{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvvdOmm/M9zlTRTg82lmEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isababale/My-Documents-/blob/main/Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autograd**\n",
        "\n",
        "Autograd is PyTorch’s automatic differentiation engine.\n",
        "It automatically calculates gradients (derivatives) for you — which are essential for training machine learning models using gradient descent."
      ],
      "metadata": {
        "id": "2Tn3sTnWFPHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aI79WJMgCiwg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "cuda0=torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([5],dtype=torch.float,requires_grad=True)\n",
        "y=torch.tensor([6],dtype=torch.float,requires_grad=True)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yfm6cJSCoyi",
        "outputId": "c76e7de1-63d4-456a-ae9c-504c964d3c52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5.], requires_grad=True)\n",
            "tensor([6.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the function\n",
        "z=((x**2)*y)+(x*y)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqaTgI97Fho_",
        "outputId": "bb50362a-0413-4319-b417-fbae43f9f3f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([180.], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usng autograd\n",
        "# autograd to be applied on scalar\n",
        "total =torch.sum(z)\n",
        "print(total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6-IDuBDDH2e",
        "outputId": "5a3c6c04-5f10-4c47-bdce-be8728560bac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(180., grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total.backward()"
      ],
      "metadata": {
        "id": "p3uS-e-6HCYv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRA-NV3HIIqU",
        "outputId": "e832122f-62ce-4093-c091-dc48dfa00d9d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([66.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7uW-bY-IPTb",
        "outputId": "33d1d70e-358f-49d8-e1b8-d6c0acd51b47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([30.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.randn(10,requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "JIE4C4KkKtQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3cde3c-9651-4b48-8214-a8b2a206184f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.2251,  0.3490,  0.5034, -0.0805, -0.5455,  1.0165, -0.2344, -0.8856,\n",
            "         0.2955, -0.6256], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building intuition on Autograd**"
      ],
      "metadata": {
        "id": "qHPzS4xes8iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.randn(10,device=cuda0)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GyzF8XStD5c",
        "outputId": "55c12649-64ec-411b-f390-823e2bff1afb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0258, -1.3162, -1.4225,  1.0516,  0.9411, -0.0402, -0.2711, -0.9333,\n",
              "         0.9416,  1.3084], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function\n",
        "y=1.8*x +32 # wx+b\n",
        "y  # pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoEAeRa6to2o",
        "outputId": "9a8a790a-3e30-4ee6-9562-6a2d9e6f6404"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([31.9535, 29.6308, 29.4395, 33.8929, 33.6940, 31.9276, 31.5120, 30.3201,\n",
              "        33.6949, 34.3550], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w=torch.ones(1,requires_grad=True,device=cuda0)\n",
        "b=torch.ones(1,requires_grad=True,device=cuda0)\n",
        "y_hat=w*x+b # predicted values of the output"
      ],
      "metadata": {
        "id": "g1giVs-6utnH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "loss=torch.sum((y_hat-y)**2)\n"
      ],
      "metadata": {
        "id": "WSiiOSK7vean"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJSc6e3wwOuf",
        "outputId": "81f7a2fb-31e0-42a2-ceb6-cd28f148d5ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9627.5312, device='cuda:0', grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "D8VRvLYzwRL9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad,b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD2s3mZGwYZQ",
        "outputId": "ef9663cb-3760-498c-b653-4239f367e2ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-29.3486], device='cuda:0') tensor([-620.3737], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementing autograd**"
      ],
      "metadata": {
        "id": "uBAttDp9wzO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.randint(-100,100,(100,), dtype=torch.float32, device=cuda0)\n",
        "y=1.8*x + 32\n",
        "w=torch.ones(1,requires_grad=True,device=cuda0)\n",
        "b=torch.ones(1,requires_grad=True,device=cuda0)\n",
        "y_hat=w*x+b\n",
        "epochs=100000\n",
        "lr=0.000001\n",
        "\n",
        "for i in range(epochs):\n",
        "  y_hat=w*x+b\n",
        "  loss=torch.sum((y_hat-y)**2)\n",
        "  loss.backward()\n",
        "  # w-=lr*w.grad -- this will be considered as relationship\n",
        "  with torch.no_grad(): # this will swith off gradiends\n",
        "    w-=lr*w.grad\n",
        "    b-=lr*b.grad\n",
        "\n",
        "    # setting the gradients to be zero\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "y_hat=w*x+b\n",
        "print(w.item(),b.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8yuN0NcxCsZ",
        "outputId": "822edb54-fe49-434d-b374-4a04717b8b9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8000075817108154 31.99519920349121\n"
          ]
        }
      ]
    }
  ]
}
