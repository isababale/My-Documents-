{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjWhtVz9GOT98OFgFH7khS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isababale/My-Documents-/blob/main/Deep_NN_PYTORCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BUILDING A DEEP NEURAL NETWORK**\n"
      ],
      "metadata": {
        "id": "wZUneApMeMXX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3jlwljYOBGVk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n_in=input layer size\n",
        "# n_h= hidden layer\n",
        "# n_out =output layer\n",
        "n_in,n_h,n_out,batch_size=10,5,1,10"
      ],
      "metadata": {
        "id": "8-GMLxgnBYHs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.randn(batch_size,n_in)\n",
        "y=torch.tensor([[1.0],[1.0],[0.0],[1.0],[0.0],[1.0],[0.0],[1.0],[1.0],[0.0]])"
      ],
      "metadata": {
        "id": "I2pj0aNOB1H_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a model\n",
        "model=nn.Sequential(\n",
        "    nn.Linear(n_in,n_h),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(n_h,n_out),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "dO1SRwkKDGjh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func=nn.MSELoss()"
      ],
      "metadata": {
        "id": "P6ThAhH7ECVk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "cJMXbCDHEKrY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descend\n",
        "for i in range(500):\n",
        "  y_pred=model(x)\n",
        "  loss=loss_func(y_pred,y)\n",
        "  print('epoch:',(i+1),'loss: ', loss.item())\n",
        "  #backward\n",
        "  loss.backward()\n",
        "\n",
        "  # update parameters\n",
        "  optimizer.step() # update all the parameters\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # this will execute until the total number of epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMogzUdsEbO5",
        "outputId": "259102f1-2c93-4a2d-ad93-1d99c32a6c9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 loss:  0.20090241730213165\n",
            "epoch: 2 loss:  0.20073123276233673\n",
            "epoch: 3 loss:  0.20055989921092987\n",
            "epoch: 4 loss:  0.20038847625255585\n",
            "epoch: 5 loss:  0.20021696388721466\n",
            "epoch: 6 loss:  0.2000453919172287\n",
            "epoch: 7 loss:  0.1998736560344696\n",
            "epoch: 8 loss:  0.19970133900642395\n",
            "epoch: 9 loss:  0.1995275467634201\n",
            "epoch: 10 loss:  0.1993536353111267\n",
            "epoch: 11 loss:  0.19917963445186615\n",
            "epoch: 12 loss:  0.19900554418563843\n",
            "epoch: 13 loss:  0.19883134961128235\n",
            "epoch: 14 loss:  0.19865702092647552\n",
            "epoch: 15 loss:  0.1984826624393463\n",
            "epoch: 16 loss:  0.19830815494060516\n",
            "epoch: 17 loss:  0.19813357293605804\n",
            "epoch: 18 loss:  0.19795890152454376\n",
            "epoch: 19 loss:  0.19778411090373993\n",
            "epoch: 20 loss:  0.19760927557945251\n",
            "epoch: 21 loss:  0.19743429124355316\n",
            "epoch: 22 loss:  0.19725924730300903\n",
            "epoch: 23 loss:  0.19708411395549774\n",
            "epoch: 24 loss:  0.1969088762998581\n",
            "epoch: 25 loss:  0.19673356413841248\n",
            "epoch: 26 loss:  0.19655819237232208\n",
            "epoch: 27 loss:  0.19638271629810333\n",
            "epoch: 28 loss:  0.19620713591575623\n",
            "epoch: 29 loss:  0.19603151082992554\n",
            "epoch: 30 loss:  0.19585579633712769\n",
            "epoch: 31 loss:  0.19567997753620148\n",
            "epoch: 32 loss:  0.1955041140317917\n",
            "epoch: 33 loss:  0.19532819092273712\n",
            "epoch: 34 loss:  0.195152148604393\n",
            "epoch: 35 loss:  0.1949760466814041\n",
            "epoch: 36 loss:  0.19479984045028687\n",
            "epoch: 37 loss:  0.19462358951568604\n",
            "epoch: 38 loss:  0.19444727897644043\n",
            "epoch: 39 loss:  0.19427087903022766\n",
            "epoch: 40 loss:  0.19409441947937012\n",
            "epoch: 41 loss:  0.1939178705215454\n",
            "epoch: 42 loss:  0.1937413215637207\n",
            "epoch: 43 loss:  0.19356462359428406\n",
            "epoch: 44 loss:  0.19338791072368622\n",
            "epoch: 45 loss:  0.19321110844612122\n",
            "epoch: 46 loss:  0.19303429126739502\n",
            "epoch: 47 loss:  0.19285735487937927\n",
            "epoch: 48 loss:  0.19268038868904114\n",
            "epoch: 49 loss:  0.19250336289405823\n",
            "epoch: 50 loss:  0.19232626259326935\n",
            "epoch: 51 loss:  0.19214913249015808\n",
            "epoch: 52 loss:  0.19197194278240204\n",
            "epoch: 53 loss:  0.19179469347000122\n",
            "epoch: 54 loss:  0.19161738455295563\n",
            "epoch: 55 loss:  0.19144003093242645\n",
            "epoch: 56 loss:  0.1912626326084137\n",
            "epoch: 57 loss:  0.19108517467975616\n",
            "epoch: 58 loss:  0.19090768694877625\n",
            "epoch: 59 loss:  0.19073013961315155\n",
            "epoch: 60 loss:  0.19055256247520447\n",
            "epoch: 61 loss:  0.1903749257326126\n",
            "epoch: 62 loss:  0.19019724428653717\n",
            "epoch: 63 loss:  0.19001951813697815\n",
            "epoch: 64 loss:  0.18984176218509674\n",
            "epoch: 65 loss:  0.18966397643089294\n",
            "epoch: 66 loss:  0.18948613107204437\n",
            "epoch: 67 loss:  0.1893082559108734\n",
            "epoch: 68 loss:  0.18913039565086365\n",
            "epoch: 69 loss:  0.18895241618156433\n",
            "epoch: 70 loss:  0.1887744516134262\n",
            "epoch: 71 loss:  0.1885964572429657\n",
            "epoch: 72 loss:  0.1884184181690216\n",
            "epoch: 73 loss:  0.18824036419391632\n",
            "epoch: 74 loss:  0.18806226551532745\n",
            "epoch: 75 loss:  0.1878841668367386\n",
            "epoch: 76 loss:  0.18770599365234375\n",
            "epoch: 77 loss:  0.1875278502702713\n",
            "epoch: 78 loss:  0.18734966218471527\n",
            "epoch: 79 loss:  0.18717142939567566\n",
            "epoch: 80 loss:  0.18699321150779724\n",
            "epoch: 81 loss:  0.18681497871875763\n",
            "epoch: 82 loss:  0.18663668632507324\n",
            "epoch: 83 loss:  0.18645839393138885\n",
            "epoch: 84 loss:  0.18628010153770447\n",
            "epoch: 85 loss:  0.1861017793416977\n",
            "epoch: 86 loss:  0.18592345714569092\n",
            "epoch: 87 loss:  0.18574512004852295\n",
            "epoch: 88 loss:  0.1855667680501938\n",
            "epoch: 89 loss:  0.18538838624954224\n",
            "epoch: 90 loss:  0.18521001935005188\n",
            "epoch: 91 loss:  0.18503163754940033\n",
            "epoch: 92 loss:  0.18485327064990997\n",
            "epoch: 93 loss:  0.18467485904693604\n",
            "epoch: 94 loss:  0.1844964623451233\n",
            "epoch: 95 loss:  0.18431809544563293\n",
            "epoch: 96 loss:  0.1841396540403366\n",
            "epoch: 97 loss:  0.18396125733852386\n",
            "epoch: 98 loss:  0.1837828904390335\n",
            "epoch: 99 loss:  0.18360447883605957\n",
            "epoch: 100 loss:  0.18342608213424683\n",
            "epoch: 101 loss:  0.18324770033359528\n",
            "epoch: 102 loss:  0.18306931853294373\n",
            "epoch: 103 loss:  0.18289095163345337\n",
            "epoch: 104 loss:  0.182712584733963\n",
            "epoch: 105 loss:  0.18253420293331146\n",
            "epoch: 106 loss:  0.1823558658361435\n",
            "epoch: 107 loss:  0.18217754364013672\n",
            "epoch: 108 loss:  0.18199920654296875\n",
            "epoch: 109 loss:  0.18182091414928436\n",
            "epoch: 110 loss:  0.18164260685443878\n",
            "epoch: 111 loss:  0.18146434426307678\n",
            "epoch: 112 loss:  0.1812860667705536\n",
            "epoch: 113 loss:  0.18110795319080353\n",
            "epoch: 114 loss:  0.18093052506446838\n",
            "epoch: 115 loss:  0.18075314164161682\n",
            "epoch: 116 loss:  0.18057572841644287\n",
            "epoch: 117 loss:  0.1803983747959137\n",
            "epoch: 118 loss:  0.1802210509777069\n",
            "epoch: 119 loss:  0.18004374206066132\n",
            "epoch: 120 loss:  0.17986643314361572\n",
            "epoch: 121 loss:  0.1796891987323761\n",
            "epoch: 122 loss:  0.17951194941997528\n",
            "epoch: 123 loss:  0.17933475971221924\n",
            "epoch: 124 loss:  0.1791575849056244\n",
            "epoch: 125 loss:  0.17898046970367432\n",
            "epoch: 126 loss:  0.17880335450172424\n",
            "epoch: 127 loss:  0.17862626910209656\n",
            "epoch: 128 loss:  0.17844925820827484\n",
            "epoch: 129 loss:  0.17827226221561432\n",
            "epoch: 130 loss:  0.1780952662229538\n",
            "epoch: 131 loss:  0.17791834473609924\n",
            "epoch: 132 loss:  0.17774146795272827\n",
            "epoch: 133 loss:  0.1775646209716797\n",
            "epoch: 134 loss:  0.17738783359527588\n",
            "epoch: 135 loss:  0.17721107602119446\n",
            "epoch: 136 loss:  0.17703434824943542\n",
            "epoch: 137 loss:  0.17685768008232117\n",
            "epoch: 138 loss:  0.1766810566186905\n",
            "epoch: 139 loss:  0.1765044778585434\n",
            "epoch: 140 loss:  0.17632794380187988\n",
            "epoch: 141 loss:  0.17615148425102234\n",
            "epoch: 142 loss:  0.175975039601326\n",
            "epoch: 143 loss:  0.17579865455627441\n",
            "epoch: 144 loss:  0.17562231421470642\n",
            "epoch: 145 loss:  0.1754460334777832\n",
            "epoch: 146 loss:  0.17526981234550476\n",
            "epoch: 147 loss:  0.1750936657190323\n",
            "epoch: 148 loss:  0.174917533993721\n",
            "epoch: 149 loss:  0.1747414767742157\n",
            "epoch: 150 loss:  0.17456546425819397\n",
            "epoch: 151 loss:  0.1743895262479782\n",
            "epoch: 152 loss:  0.17421367764472961\n",
            "epoch: 153 loss:  0.1740378439426422\n",
            "epoch: 154 loss:  0.17386206984519958\n",
            "epoch: 155 loss:  0.17368637025356293\n",
            "epoch: 156 loss:  0.17351076006889343\n",
            "epoch: 157 loss:  0.17333516478538513\n",
            "epoch: 158 loss:  0.17315968871116638\n",
            "epoch: 159 loss:  0.17298424243927002\n",
            "epoch: 160 loss:  0.17280885577201843\n",
            "epoch: 161 loss:  0.172633558511734\n",
            "epoch: 162 loss:  0.17245832085609436\n",
            "epoch: 163 loss:  0.17228315770626068\n",
            "epoch: 164 loss:  0.17210806906223297\n",
            "epoch: 165 loss:  0.17193304002285004\n",
            "epoch: 166 loss:  0.17175810039043427\n",
            "epoch: 167 loss:  0.17158322036266327\n",
            "epoch: 168 loss:  0.17140842974185944\n",
            "epoch: 169 loss:  0.17123369872570038\n",
            "epoch: 170 loss:  0.1710590422153473\n",
            "epoch: 171 loss:  0.17088447511196136\n",
            "epoch: 172 loss:  0.1707099825143814\n",
            "epoch: 173 loss:  0.17053554952144623\n",
            "epoch: 174 loss:  0.1703612357378006\n",
            "epoch: 175 loss:  0.17018696665763855\n",
            "epoch: 176 loss:  0.17001278698444366\n",
            "epoch: 177 loss:  0.16983869671821594\n",
            "epoch: 178 loss:  0.1696646809577942\n",
            "epoch: 179 loss:  0.1694907546043396\n",
            "epoch: 180 loss:  0.16931691765785217\n",
            "epoch: 181 loss:  0.1691431701183319\n",
            "epoch: 182 loss:  0.16896948218345642\n",
            "epoch: 183 loss:  0.1687958985567093\n",
            "epoch: 184 loss:  0.16862238943576813\n",
            "epoch: 185 loss:  0.1684490144252777\n",
            "epoch: 186 loss:  0.16827566921710968\n",
            "epoch: 187 loss:  0.1681024432182312\n",
            "epoch: 188 loss:  0.16792932152748108\n",
            "epoch: 189 loss:  0.16775625944137573\n",
            "epoch: 190 loss:  0.16758330166339874\n",
            "epoch: 191 loss:  0.1674104481935501\n",
            "epoch: 192 loss:  0.16723768413066864\n",
            "epoch: 193 loss:  0.16706500947475433\n",
            "epoch: 194 loss:  0.1668924242258072\n",
            "epoch: 195 loss:  0.1667199432849884\n",
            "epoch: 196 loss:  0.16654756665229797\n",
            "epoch: 197 loss:  0.1663752645254135\n",
            "epoch: 198 loss:  0.1662030667066574\n",
            "epoch: 199 loss:  0.16603098809719086\n",
            "epoch: 200 loss:  0.16585899889469147\n",
            "epoch: 201 loss:  0.16568709909915924\n",
            "epoch: 202 loss:  0.16551531851291656\n",
            "epoch: 203 loss:  0.16534362733364105\n",
            "epoch: 204 loss:  0.1651720255613327\n",
            "epoch: 205 loss:  0.1650005578994751\n",
            "epoch: 206 loss:  0.16482917964458466\n",
            "epoch: 207 loss:  0.16465792059898376\n",
            "epoch: 208 loss:  0.16448672115802765\n",
            "epoch: 209 loss:  0.16431565582752228\n",
            "epoch: 210 loss:  0.16414467990398407\n",
            "epoch: 211 loss:  0.1639738529920578\n",
            "epoch: 212 loss:  0.1638030707836151\n",
            "epoch: 213 loss:  0.16363243758678436\n",
            "epoch: 214 loss:  0.16346190869808197\n",
            "epoch: 215 loss:  0.16329149901866913\n",
            "epoch: 216 loss:  0.16312119364738464\n",
            "epoch: 217 loss:  0.16295099258422852\n",
            "epoch: 218 loss:  0.16278091073036194\n",
            "epoch: 219 loss:  0.16261091828346252\n",
            "epoch: 220 loss:  0.16244107484817505\n",
            "epoch: 221 loss:  0.16227130591869354\n",
            "epoch: 222 loss:  0.1621016561985016\n",
            "epoch: 223 loss:  0.16193214058876038\n",
            "epoch: 224 loss:  0.16176274418830872\n",
            "epoch: 225 loss:  0.16159343719482422\n",
            "epoch: 226 loss:  0.16142424941062927\n",
            "epoch: 227 loss:  0.16125521063804626\n",
            "epoch: 228 loss:  0.16108623147010803\n",
            "epoch: 229 loss:  0.16091743111610413\n",
            "epoch: 230 loss:  0.160748690366745\n",
            "epoch: 231 loss:  0.1605800986289978\n",
            "epoch: 232 loss:  0.16041162610054016\n",
            "epoch: 233 loss:  0.16024327278137207\n",
            "epoch: 234 loss:  0.16007502377033234\n",
            "epoch: 235 loss:  0.15990690886974335\n",
            "epoch: 236 loss:  0.1597389280796051\n",
            "epoch: 237 loss:  0.15957103669643402\n",
            "epoch: 238 loss:  0.15940329432487488\n",
            "epoch: 239 loss:  0.15923567116260529\n",
            "epoch: 240 loss:  0.15906816720962524\n",
            "epoch: 241 loss:  0.15890078246593475\n",
            "epoch: 242 loss:  0.1587335169315338\n",
            "epoch: 243 loss:  0.15856637060642242\n",
            "epoch: 244 loss:  0.15839937329292297\n",
            "epoch: 245 loss:  0.15823248028755188\n",
            "epoch: 246 loss:  0.15806570649147034\n",
            "epoch: 247 loss:  0.15789908170700073\n",
            "epoch: 248 loss:  0.15773256123065948\n",
            "epoch: 249 loss:  0.15756618976593018\n",
            "epoch: 250 loss:  0.15739992260932922\n",
            "epoch: 251 loss:  0.15723378956317902\n",
            "epoch: 252 loss:  0.15706779062747955\n",
            "epoch: 253 loss:  0.15690192580223083\n",
            "epoch: 254 loss:  0.15673616528511047\n",
            "epoch: 255 loss:  0.15657055377960205\n",
            "epoch: 256 loss:  0.15640506148338318\n",
            "epoch: 257 loss:  0.15623970329761505\n",
            "epoch: 258 loss:  0.15607449412345886\n",
            "epoch: 259 loss:  0.15590937435626984\n",
            "epoch: 260 loss:  0.15574441850185394\n",
            "epoch: 261 loss:  0.1555795669555664\n",
            "epoch: 262 loss:  0.155414879322052\n",
            "epoch: 263 loss:  0.15525031089782715\n",
            "epoch: 264 loss:  0.15508584678173065\n",
            "epoch: 265 loss:  0.1549215465784073\n",
            "epoch: 266 loss:  0.15475738048553467\n",
            "epoch: 267 loss:  0.1545933336019516\n",
            "epoch: 268 loss:  0.15442942082881927\n",
            "epoch: 269 loss:  0.1542656570672989\n",
            "epoch: 270 loss:  0.15410202741622925\n",
            "epoch: 271 loss:  0.15393850207328796\n",
            "epoch: 272 loss:  0.153775155544281\n",
            "epoch: 273 loss:  0.1536118984222412\n",
            "epoch: 274 loss:  0.15344882011413574\n",
            "epoch: 275 loss:  0.15328586101531982\n",
            "epoch: 276 loss:  0.15312300622463226\n",
            "epoch: 277 loss:  0.15296033024787903\n",
            "epoch: 278 loss:  0.15279777348041534\n",
            "epoch: 279 loss:  0.1526353508234024\n",
            "epoch: 280 loss:  0.1524730622768402\n",
            "epoch: 281 loss:  0.15231092274188995\n",
            "epoch: 282 loss:  0.15214891731739044\n",
            "epoch: 283 loss:  0.15198703110218048\n",
            "epoch: 284 loss:  0.15182527899742126\n",
            "epoch: 285 loss:  0.15166370570659637\n",
            "epoch: 286 loss:  0.15150228142738342\n",
            "epoch: 287 loss:  0.15134093165397644\n",
            "epoch: 288 loss:  0.15117976069450378\n",
            "epoch: 289 loss:  0.15101873874664307\n",
            "epoch: 290 loss:  0.1508578360080719\n",
            "epoch: 291 loss:  0.15069708228111267\n",
            "epoch: 292 loss:  0.150536447763443\n",
            "epoch: 293 loss:  0.15037596225738525\n",
            "epoch: 294 loss:  0.15021561086177826\n",
            "epoch: 295 loss:  0.1500554084777832\n",
            "epoch: 296 loss:  0.14989535510540009\n",
            "epoch: 297 loss:  0.14973698556423187\n",
            "epoch: 298 loss:  0.14958378672599792\n",
            "epoch: 299 loss:  0.14943072199821472\n",
            "epoch: 300 loss:  0.14927779138088226\n",
            "epoch: 301 loss:  0.14912500977516174\n",
            "epoch: 302 loss:  0.14897236227989197\n",
            "epoch: 303 loss:  0.14881983399391174\n",
            "epoch: 304 loss:  0.14866743981838226\n",
            "epoch: 305 loss:  0.14851519465446472\n",
            "epoch: 306 loss:  0.14836309850215912\n",
            "epoch: 307 loss:  0.14821110665798187\n",
            "epoch: 308 loss:  0.14805927872657776\n",
            "epoch: 309 loss:  0.1479075849056244\n",
            "epoch: 310 loss:  0.14775601029396057\n",
            "epoch: 311 loss:  0.1476045697927475\n",
            "epoch: 312 loss:  0.14745327830314636\n",
            "epoch: 313 loss:  0.14730212092399597\n",
            "epoch: 314 loss:  0.14715109765529633\n",
            "epoch: 315 loss:  0.14700020849704742\n",
            "epoch: 316 loss:  0.14684946835041046\n",
            "epoch: 317 loss:  0.14669881761074066\n",
            "epoch: 318 loss:  0.14654836058616638\n",
            "epoch: 319 loss:  0.14639799296855927\n",
            "epoch: 320 loss:  0.14624778926372528\n",
            "epoch: 321 loss:  0.14609768986701965\n",
            "epoch: 322 loss:  0.14594776928424835\n",
            "epoch: 323 loss:  0.1457979381084442\n",
            "epoch: 324 loss:  0.1456482708454132\n",
            "epoch: 325 loss:  0.14549872279167175\n",
            "epoch: 326 loss:  0.14534933865070343\n",
            "epoch: 327 loss:  0.1452009677886963\n",
            "epoch: 328 loss:  0.1450527012348175\n",
            "epoch: 329 loss:  0.14490458369255066\n",
            "epoch: 330 loss:  0.14475658535957336\n",
            "epoch: 331 loss:  0.14460870623588562\n",
            "epoch: 332 loss:  0.14446094632148743\n",
            "epoch: 333 loss:  0.14431336522102356\n",
            "epoch: 334 loss:  0.14416584372520447\n",
            "epoch: 335 loss:  0.1440184861421585\n",
            "epoch: 336 loss:  0.1438712626695633\n",
            "epoch: 337 loss:  0.14372412860393524\n",
            "epoch: 338 loss:  0.14357717335224152\n",
            "epoch: 339 loss:  0.14343029260635376\n",
            "epoch: 340 loss:  0.14328357577323914\n",
            "epoch: 341 loss:  0.14313697814941406\n",
            "epoch: 342 loss:  0.14299049973487854\n",
            "epoch: 343 loss:  0.14284417033195496\n",
            "epoch: 344 loss:  0.14269794523715973\n",
            "epoch: 345 loss:  0.14255185425281525\n",
            "epoch: 346 loss:  0.1424058973789215\n",
            "epoch: 347 loss:  0.14226004481315613\n",
            "epoch: 348 loss:  0.1421143263578415\n",
            "epoch: 349 loss:  0.1419687271118164\n",
            "epoch: 350 loss:  0.14182326197624207\n",
            "epoch: 351 loss:  0.14167793095111847\n",
            "epoch: 352 loss:  0.14153273403644562\n",
            "epoch: 353 loss:  0.14138764142990112\n",
            "epoch: 354 loss:  0.14124269783496857\n",
            "epoch: 355 loss:  0.14109784364700317\n",
            "epoch: 356 loss:  0.14095313847064972\n",
            "epoch: 357 loss:  0.14080855250358582\n",
            "epoch: 358 loss:  0.14066407084465027\n",
            "epoch: 359 loss:  0.14051973819732666\n",
            "epoch: 360 loss:  0.1403755396604538\n",
            "epoch: 361 loss:  0.1402314454317093\n",
            "epoch: 362 loss:  0.14008745551109314\n",
            "epoch: 363 loss:  0.13994362950325012\n",
            "epoch: 364 loss:  0.13979992270469666\n",
            "epoch: 365 loss:  0.13965633511543274\n",
            "epoch: 366 loss:  0.13951286673545837\n",
            "epoch: 367 loss:  0.13936951756477356\n",
            "epoch: 368 loss:  0.1392262727022171\n",
            "epoch: 369 loss:  0.13908317685127258\n",
            "epoch: 370 loss:  0.1389402151107788\n",
            "epoch: 371 loss:  0.13879737257957458\n",
            "epoch: 372 loss:  0.13865463435649872\n",
            "epoch: 373 loss:  0.1385120451450348\n",
            "epoch: 374 loss:  0.13836956024169922\n",
            "epoch: 375 loss:  0.1382271945476532\n",
            "epoch: 376 loss:  0.13808493316173553\n",
            "epoch: 377 loss:  0.1379428207874298\n",
            "epoch: 378 loss:  0.13780081272125244\n",
            "epoch: 379 loss:  0.137658953666687\n",
            "epoch: 380 loss:  0.13751718401908875\n",
            "epoch: 381 loss:  0.13737556338310242\n",
            "epoch: 382 loss:  0.13723404705524445\n",
            "epoch: 383 loss:  0.13709266483783722\n",
            "epoch: 384 loss:  0.13695137202739716\n",
            "epoch: 385 loss:  0.13681024312973022\n",
            "epoch: 386 loss:  0.13666921854019165\n",
            "epoch: 387 loss:  0.13652829825878143\n",
            "epoch: 388 loss:  0.13638754189014435\n",
            "epoch: 389 loss:  0.13624684512615204\n",
            "epoch: 390 loss:  0.13610629737377167\n",
            "epoch: 391 loss:  0.13596586883068085\n",
            "epoch: 392 loss:  0.13582557439804077\n",
            "epoch: 393 loss:  0.13568538427352905\n",
            "epoch: 394 loss:  0.13554532825946808\n",
            "epoch: 395 loss:  0.13540536165237427\n",
            "epoch: 396 loss:  0.1352655440568924\n",
            "epoch: 397 loss:  0.13512583076953888\n",
            "epoch: 398 loss:  0.13498620688915253\n",
            "epoch: 399 loss:  0.1348467469215393\n",
            "epoch: 400 loss:  0.13470740616321564\n",
            "epoch: 401 loss:  0.13456813991069794\n",
            "epoch: 402 loss:  0.13442905247211456\n",
            "epoch: 403 loss:  0.13429002463817596\n",
            "epoch: 404 loss:  0.1341511458158493\n",
            "epoch: 405 loss:  0.1340123862028122\n",
            "epoch: 406 loss:  0.13387373089790344\n",
            "epoch: 407 loss:  0.13373519480228424\n",
            "epoch: 408 loss:  0.1335967630147934\n",
            "epoch: 409 loss:  0.1334584802389145\n",
            "epoch: 410 loss:  0.13332030177116394\n",
            "epoch: 411 loss:  0.13318224251270294\n",
            "epoch: 412 loss:  0.1330442875623703\n",
            "epoch: 413 loss:  0.1329064518213272\n",
            "epoch: 414 loss:  0.13276872038841248\n",
            "epoch: 415 loss:  0.1326311230659485\n",
            "epoch: 416 loss:  0.13249364495277405\n",
            "epoch: 417 loss:  0.13235625624656677\n",
            "epoch: 418 loss:  0.13221898674964905\n",
            "epoch: 419 loss:  0.13208183646202087\n",
            "epoch: 420 loss:  0.13194480538368225\n",
            "epoch: 421 loss:  0.13180789351463318\n",
            "epoch: 422 loss:  0.13167110085487366\n",
            "epoch: 423 loss:  0.13155116140842438\n",
            "epoch: 424 loss:  0.13143137097358704\n",
            "epoch: 425 loss:  0.13131175935268402\n",
            "epoch: 426 loss:  0.13119234144687653\n",
            "epoch: 427 loss:  0.13107311725616455\n",
            "epoch: 428 loss:  0.1309540718793869\n",
            "epoch: 429 loss:  0.13083520531654358\n",
            "epoch: 430 loss:  0.13071857392787933\n",
            "epoch: 431 loss:  0.130600705742836\n",
            "epoch: 432 loss:  0.13048236072063446\n",
            "epoch: 433 loss:  0.13036422431468964\n",
            "epoch: 434 loss:  0.13024628162384033\n",
            "epoch: 435 loss:  0.13012848794460297\n",
            "epoch: 436 loss:  0.13001088798046112\n",
            "epoch: 437 loss:  0.1298934817314148\n",
            "epoch: 438 loss:  0.1297772377729416\n",
            "epoch: 439 loss:  0.12966184318065643\n",
            "epoch: 440 loss:  0.1295449435710907\n",
            "epoch: 441 loss:  0.12942823767662048\n",
            "epoch: 442 loss:  0.1293117105960846\n",
            "epoch: 443 loss:  0.12919537723064423\n",
            "epoch: 444 loss:  0.1290791928768158\n",
            "epoch: 445 loss:  0.12896320223808289\n",
            "epoch: 446 loss:  0.12884736061096191\n",
            "epoch: 447 loss:  0.12873384356498718\n",
            "epoch: 448 loss:  0.12861891090869904\n",
            "epoch: 449 loss:  0.12850362062454224\n",
            "epoch: 450 loss:  0.12838849425315857\n",
            "epoch: 451 loss:  0.12827353179454803\n",
            "epoch: 452 loss:  0.12815876305103302\n",
            "epoch: 453 loss:  0.12804417312145233\n",
            "epoch: 454 loss:  0.12792974710464478\n",
            "epoch: 455 loss:  0.1278155893087387\n",
            "epoch: 456 loss:  0.12770403921604156\n",
            "epoch: 457 loss:  0.1275901049375534\n",
            "epoch: 458 loss:  0.12747636437416077\n",
            "epoch: 459 loss:  0.12736280262470245\n",
            "epoch: 460 loss:  0.12724938988685608\n",
            "epoch: 461 loss:  0.12713617086410522\n",
            "epoch: 462 loss:  0.1270231157541275\n",
            "epoch: 463 loss:  0.12691020965576172\n",
            "epoch: 464 loss:  0.12679770588874817\n",
            "epoch: 465 loss:  0.12668755650520325\n",
            "epoch: 466 loss:  0.12657517194747925\n",
            "epoch: 467 loss:  0.12646295130252838\n",
            "epoch: 468 loss:  0.12635087966918945\n",
            "epoch: 469 loss:  0.12623898684978485\n",
            "epoch: 470 loss:  0.12612728774547577\n",
            "epoch: 471 loss:  0.12601572275161743\n",
            "epoch: 472 loss:  0.12590433657169342\n",
            "epoch: 473 loss:  0.12579312920570374\n",
            "epoch: 474 loss:  0.12568442523479462\n",
            "epoch: 475 loss:  0.1255737692117691\n",
            "epoch: 476 loss:  0.12546302378177643\n",
            "epoch: 477 loss:  0.12535245716571808\n",
            "epoch: 478 loss:  0.12524202466011047\n",
            "epoch: 479 loss:  0.12513180077075958\n",
            "epoch: 480 loss:  0.12502172589302063\n",
            "epoch: 481 loss:  0.12491180002689362\n",
            "epoch: 482 loss:  0.12480205297470093\n",
            "epoch: 483 loss:  0.12469382584095001\n",
            "epoch: 484 loss:  0.1245855838060379\n",
            "epoch: 485 loss:  0.12447629123926163\n",
            "epoch: 486 loss:  0.12436716258525848\n",
            "epoch: 487 loss:  0.12425823509693146\n",
            "epoch: 488 loss:  0.12414941936731339\n",
            "epoch: 489 loss:  0.12404079735279083\n",
            "epoch: 490 loss:  0.12393228709697723\n",
            "epoch: 491 loss:  0.12382400035858154\n",
            "epoch: 492 loss:  0.12371580302715302\n",
            "epoch: 493 loss:  0.12361013889312744\n",
            "epoch: 494 loss:  0.1235024705529213\n",
            "epoch: 495 loss:  0.12339477241039276\n",
            "epoch: 496 loss:  0.12328723818063736\n",
            "epoch: 497 loss:  0.1231798380613327\n",
            "epoch: 498 loss:  0.12307262420654297\n",
            "epoch: 499 loss:  0.12296553701162338\n",
            "epoch: 500 loss:  0.12285862118005753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Custom NN Module**\n",
        "\n",
        "With the use of torch.nn.module we can combine many simple layers to implement complex neural networks.\n",
        "in other words we can use it to represent an arbitrary function f in pytorch"
      ],
      "metadata": {
        "id": "LW90J9WDGkJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "we subclass torch.nn.module for complex networks.\n",
        "we override methods under --> torch.nn.module\n",
        "\n",
        "1. __init__ function:\n",
        "     -invoked when we create instance of nn.module\n",
        "2. forward function:\n",
        "   -we define how output will be computed\n",
        "   \"\"\""
      ],
      "metadata": {
        "id": "DwIwqCdEHTt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_Class(nn.Module):\n",
        "  def __init__(self,n_in,n_h,n_out):\n",
        "    super(Custom_Class,self).__init__()\n",
        "    self.model=nn.Sequential(nn.Linear(n_in,n_h),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Linear( n_h,n_out),\n",
        "                            nn.Sigmoid())\n",
        "  def forward(self,x):\n",
        "   return self.model(x)\n"
      ],
      "metadata": {
        "id": "HgGY4qUlIrH1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_nn=Custom_Class(n_in,n_h,n_out)"
      ],
      "metadata": {
        "id": "UvOO7Q0vKVr3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD(custom_nn.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "HDht6WJuKvNN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for i in range(500):\n",
        "  y_pred=custom_nn(x) # instead of model(x) we replace with custom_nn(x)\n",
        "  loss=loss_func(y_pred,y)\n",
        "  print('epoch: ', (i+1), 'loss: ', loss.item())\n",
        "\n",
        "  # backward\n",
        "  loss.backward()\n",
        "\n",
        "  #update the parameters\n",
        "  optimizer.step()  # update all parameters\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # this will execute until the total number of epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6kvlhjcLQnK",
        "outputId": "3c23c54d-833c-48d4-8093-ef338e480c81"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  1 loss:  0.1390012800693512\n",
            "epoch:  2 loss:  0.13882607221603394\n",
            "epoch:  3 loss:  0.13865694403648376\n",
            "epoch:  4 loss:  0.1384809911251068\n",
            "epoch:  5 loss:  0.13831201195716858\n",
            "epoch:  6 loss:  0.13813425600528717\n",
            "epoch:  7 loss:  0.1379677951335907\n",
            "epoch:  8 loss:  0.1377914845943451\n",
            "epoch:  9 loss:  0.13762348890304565\n",
            "epoch:  10 loss:  0.13744761049747467\n",
            "epoch:  11 loss:  0.13727818429470062\n",
            "epoch:  12 loss:  0.13710595667362213\n",
            "epoch:  13 loss:  0.13693520426750183\n",
            "epoch:  14 loss:  0.13676373660564423\n",
            "epoch:  15 loss:  0.13659071922302246\n",
            "epoch:  16 loss:  0.13642191886901855\n",
            "epoch:  17 loss:  0.13624876737594604\n",
            "epoch:  18 loss:  0.136082261800766\n",
            "epoch:  19 loss:  0.13590632379055023\n",
            "epoch:  20 loss:  0.1357404887676239\n",
            "epoch:  21 loss:  0.13556674122810364\n",
            "epoch:  22 loss:  0.13539978861808777\n",
            "epoch:  23 loss:  0.13522788882255554\n",
            "epoch:  24 loss:  0.13505768775939941\n",
            "epoch:  25 loss:  0.13488875329494476\n",
            "epoch:  26 loss:  0.13471759855747223\n",
            "epoch:  27 loss:  0.1345520317554474\n",
            "epoch:  28 loss:  0.13437755405902863\n",
            "epoch:  29 loss:  0.13421344757080078\n",
            "epoch:  30 loss:  0.13403987884521484\n",
            "epoch:  31 loss:  0.1338750422000885\n",
            "epoch:  32 loss:  0.13370488584041595\n",
            "epoch:  33 loss:  0.1335354447364807\n",
            "epoch:  34 loss:  0.13336846232414246\n",
            "epoch:  35 loss:  0.1331963986158371\n",
            "epoch:  36 loss:  0.1330345869064331\n",
            "epoch:  37 loss:  0.13286158442497253\n",
            "epoch:  38 loss:  0.13269732892513275\n",
            "epoch:  39 loss:  0.1325269490480423\n",
            "epoch:  40 loss:  0.13235987722873688\n",
            "epoch:  41 loss:  0.1321946680545807\n",
            "epoch:  42 loss:  0.13202401995658875\n",
            "epoch:  43 loss:  0.13186118006706238\n",
            "epoch:  44 loss:  0.1316893994808197\n",
            "epoch:  45 loss:  0.13152669370174408\n",
            "epoch:  46 loss:  0.13135887682437897\n",
            "epoch:  47 loss:  0.1311921775341034\n",
            "epoch:  48 loss:  0.13102704286575317\n",
            "epoch:  49 loss:  0.13085681200027466\n",
            "epoch:  50 loss:  0.13069622218608856\n",
            "epoch:  51 loss:  0.1305270493030548\n",
            "epoch:  52 loss:  0.13036391139030457\n",
            "epoch:  53 loss:  0.13019701838493347\n",
            "epoch:  54 loss:  0.13002996146678925\n",
            "epoch:  55 loss:  0.12986770272254944\n",
            "epoch:  56 loss:  0.12969955801963806\n",
            "epoch:  57 loss:  0.12953917682170868\n",
            "epoch:  58 loss:  0.12937107682228088\n",
            "epoch:  59 loss:  0.12920670211315155\n",
            "epoch:  60 loss:  0.12904338538646698\n",
            "epoch:  61 loss:  0.12887631356716156\n",
            "epoch:  62 loss:  0.12871798872947693\n",
            "epoch:  63 loss:  0.1285494863986969\n",
            "epoch:  64 loss:  0.12838703393936157\n",
            "epoch:  65 loss:  0.12822338938713074\n",
            "epoch:  66 loss:  0.12805770337581635\n",
            "epoch:  67 loss:  0.12789973616600037\n",
            "epoch:  68 loss:  0.12773209810256958\n",
            "epoch:  69 loss:  0.12757109105587006\n",
            "epoch:  70 loss:  0.1274077147245407\n",
            "epoch:  71 loss:  0.12724259495735168\n",
            "epoch:  72 loss:  0.12708565592765808\n",
            "epoch:  73 loss:  0.1269190013408661\n",
            "epoch:  74 loss:  0.126758873462677\n",
            "epoch:  75 loss:  0.1265963613986969\n",
            "epoch:  76 loss:  0.1264311969280243\n",
            "epoch:  77 loss:  0.12627597153186798\n",
            "epoch:  78 loss:  0.12611015141010284\n",
            "epoch:  79 loss:  0.12595024704933167\n",
            "epoch:  80 loss:  0.12578926980495453\n",
            "epoch:  81 loss:  0.12562410533428192\n",
            "epoch:  82 loss:  0.1254698932170868\n",
            "epoch:  83 loss:  0.12530571222305298\n",
            "epoch:  84 loss:  0.12514546513557434\n",
            "epoch:  85 loss:  0.12498648464679718\n",
            "epoch:  86 loss:  0.12482206523418427\n",
            "epoch:  87 loss:  0.12466665357351303\n",
            "epoch:  88 loss:  0.12450559437274933\n",
            "epoch:  89 loss:  0.12434442341327667\n",
            "epoch:  90 loss:  0.1241881474852562\n",
            "epoch:  91 loss:  0.12402462959289551\n",
            "epoch:  92 loss:  0.12386784702539444\n",
            "epoch:  93 loss:  0.12370876967906952\n",
            "epoch:  94 loss:  0.12354709208011627\n",
            "epoch:  95 loss:  0.12339415401220322\n",
            "epoch:  96 loss:  0.12323156744241714\n",
            "epoch:  97 loss:  0.12307392060756683\n",
            "epoch:  98 loss:  0.12292058765888214\n",
            "epoch:  99 loss:  0.12276212126016617\n",
            "epoch:  100 loss:  0.12261054664850235\n",
            "epoch:  101 loss:  0.12245434522628784\n",
            "epoch:  102 loss:  0.12229589372873306\n",
            "epoch:  103 loss:  0.12214668840169907\n",
            "epoch:  104 loss:  0.12198813259601593\n",
            "epoch:  105 loss:  0.1218344122171402\n",
            "epoch:  106 loss:  0.12168290466070175\n",
            "epoch:  107 loss:  0.12152484804391861\n",
            "epoch:  108 loss:  0.12137329578399658\n",
            "epoch:  109 loss:  0.12121925503015518\n",
            "epoch:  110 loss:  0.12106172740459442\n",
            "epoch:  111 loss:  0.12091473489999771\n",
            "epoch:  112 loss:  0.12075849622488022\n",
            "epoch:  113 loss:  0.12060312181711197\n",
            "epoch:  114 loss:  0.12045441567897797\n",
            "epoch:  115 loss:  0.12029776722192764\n",
            "epoch:  116 loss:  0.12014539539813995\n",
            "epoch:  117 loss:  0.11999623477458954\n",
            "epoch:  118 loss:  0.1198401004076004\n",
            "epoch:  119 loss:  0.11969031393527985\n",
            "epoch:  120 loss:  0.11953805387020111\n",
            "epoch:  121 loss:  0.11938245594501495\n",
            "epoch:  122 loss:  0.11923517286777496\n",
            "epoch:  123 loss:  0.11908282339572906\n",
            "epoch:  124 loss:  0.11892813444137573\n",
            "epoch:  125 loss:  0.11878256499767303\n",
            "epoch:  126 loss:  0.11862778663635254\n",
            "epoch:  127 loss:  0.11847518384456635\n",
            "epoch:  128 loss:  0.11832842975854874\n",
            "epoch:  129 loss:  0.118175208568573\n",
            "epoch:  130 loss:  0.11802508682012558\n",
            "epoch:  131 loss:  0.11787731945514679\n",
            "epoch:  132 loss:  0.11772360652685165\n",
            "epoch:  133 loss:  0.11757485568523407\n",
            "epoch:  134 loss:  0.11742627620697021\n",
            "epoch:  135 loss:  0.11727376282215118\n",
            "epoch:  136 loss:  0.11712734401226044\n",
            "epoch:  137 loss:  0.11697818338871002\n",
            "epoch:  138 loss:  0.11682555824518204\n",
            "epoch:  139 loss:  0.11667992919683456\n",
            "epoch:  140 loss:  0.11653025448322296\n",
            "epoch:  141 loss:  0.11637827008962631\n",
            "epoch:  142 loss:  0.11623511463403702\n",
            "epoch:  143 loss:  0.11608515679836273\n",
            "epoch:  144 loss:  0.11593358218669891\n",
            "epoch:  145 loss:  0.11579042673110962\n",
            "epoch:  146 loss:  0.11564038693904877\n",
            "epoch:  147 loss:  0.11548930406570435\n",
            "epoch:  148 loss:  0.11534784734249115\n",
            "epoch:  149 loss:  0.11519835889339447\n",
            "epoch:  150 loss:  0.11504777520895004\n",
            "epoch:  151 loss:  0.11490622907876968\n",
            "epoch:  152 loss:  0.11475666612386703\n",
            "epoch:  153 loss:  0.11460661888122559\n",
            "epoch:  154 loss:  0.11446571350097656\n",
            "epoch:  155 loss:  0.11431770026683807\n",
            "epoch:  156 loss:  0.114168182015419\n",
            "epoch:  157 loss:  0.11402752250432968\n",
            "epoch:  158 loss:  0.11387898772954941\n",
            "epoch:  159 loss:  0.11373009532690048\n",
            "epoch:  160 loss:  0.11358966678380966\n",
            "epoch:  161 loss:  0.11344240605831146\n",
            "epoch:  162 loss:  0.11329470574855804\n",
            "epoch:  163 loss:  0.11315426975488663\n",
            "epoch:  164 loss:  0.1130075454711914\n",
            "epoch:  165 loss:  0.11285962164402008\n",
            "epoch:  166 loss:  0.1127191036939621\n",
            "epoch:  167 loss:  0.11257358640432358\n",
            "epoch:  168 loss:  0.11242689192295074\n",
            "epoch:  169 loss:  0.11228643357753754\n",
            "epoch:  170 loss:  0.11214227974414825\n",
            "epoch:  171 loss:  0.1119953840970993\n",
            "epoch:  172 loss:  0.11185391247272491\n",
            "epoch:  173 loss:  0.11171136796474457\n",
            "epoch:  174 loss:  0.11156505346298218\n",
            "epoch:  175 loss:  0.11142361164093018\n",
            "epoch:  176 loss:  0.11128313839435577\n",
            "epoch:  177 loss:  0.11113731563091278\n",
            "epoch:  178 loss:  0.11099421977996826\n",
            "epoch:  179 loss:  0.11085526645183563\n",
            "epoch:  180 loss:  0.11070998758077621\n",
            "epoch:  181 loss:  0.1105659008026123\n",
            "epoch:  182 loss:  0.11042966693639755\n",
            "epoch:  183 loss:  0.11028535664081573\n",
            "epoch:  184 loss:  0.11014074087142944\n",
            "epoch:  185 loss:  0.11000452935695648\n",
            "epoch:  186 loss:  0.1098610907793045\n",
            "epoch:  187 loss:  0.10971702635288239\n",
            "epoch:  188 loss:  0.1095794066786766\n",
            "epoch:  189 loss:  0.10943887382745743\n",
            "epoch:  190 loss:  0.10929600894451141\n",
            "epoch:  191 loss:  0.10915664583444595\n",
            "epoch:  192 loss:  0.10901834070682526\n",
            "epoch:  193 loss:  0.1088753193616867\n",
            "epoch:  194 loss:  0.108734130859375\n",
            "epoch:  195 loss:  0.1085987538099289\n",
            "epoch:  196 loss:  0.10845635086297989\n",
            "epoch:  197 loss:  0.10831505060195923\n",
            "epoch:  198 loss:  0.10818085819482803\n",
            "epoch:  199 loss:  0.10803977400064468\n",
            "epoch:  200 loss:  0.10789798200130463\n",
            "epoch:  201 loss:  0.10776157677173615\n",
            "epoch:  202 loss:  0.10762375593185425\n",
            "epoch:  203 loss:  0.10748255252838135\n",
            "epoch:  204 loss:  0.10734432935714722\n",
            "epoch:  205 loss:  0.10721032321453094\n",
            "epoch:  206 loss:  0.10706957429647446\n",
            "epoch:  207 loss:  0.10692901909351349\n",
            "epoch:  208 loss:  0.10679646581411362\n",
            "epoch:  209 loss:  0.10665712505578995\n",
            "epoch:  210 loss:  0.10651715099811554\n",
            "epoch:  211 loss:  0.10638181120157242\n",
            "epoch:  212 loss:  0.10624723136425018\n",
            "epoch:  213 loss:  0.10610773414373398\n",
            "epoch:  214 loss:  0.10596944391727448\n",
            "epoch:  215 loss:  0.10583784431219101\n",
            "epoch:  216 loss:  0.1056988388299942\n",
            "epoch:  217 loss:  0.10556010156869888\n",
            "epoch:  218 loss:  0.10542740672826767\n",
            "epoch:  219 loss:  0.10529176145792007\n",
            "epoch:  220 loss:  0.10515423864126205\n",
            "epoch:  221 loss:  0.10501817613840103\n",
            "epoch:  222 loss:  0.10488668829202652\n",
            "epoch:  223 loss:  0.10474888980388641\n",
            "epoch:  224 loss:  0.1046113595366478\n",
            "epoch:  225 loss:  0.10448044538497925\n",
            "epoch:  226 loss:  0.1043451651930809\n",
            "epoch:  227 loss:  0.10420830547809601\n",
            "epoch:  228 loss:  0.10407433658838272\n",
            "epoch:  229 loss:  0.1039438396692276\n",
            "epoch:  230 loss:  0.1038072481751442\n",
            "epoch:  231 loss:  0.1036708801984787\n",
            "epoch:  232 loss:  0.10354089736938477\n",
            "epoch:  233 loss:  0.10340707004070282\n",
            "epoch:  234 loss:  0.10327117145061493\n",
            "epoch:  235 loss:  0.10313709825277328\n",
            "epoch:  236 loss:  0.10300922393798828\n",
            "epoch:  237 loss:  0.10287384688854218\n",
            "epoch:  238 loss:  0.10273866355419159\n",
            "epoch:  239 loss:  0.10260872542858124\n",
            "epoch:  240 loss:  0.10247717052698135\n",
            "epoch:  241 loss:  0.10234250128269196\n",
            "epoch:  242 loss:  0.1022079735994339\n",
            "epoch:  243 loss:  0.10208191722631454\n",
            "epoch:  244 loss:  0.10194853693246841\n",
            "epoch:  245 loss:  0.10181467235088348\n",
            "epoch:  246 loss:  0.10168395191431046\n",
            "epoch:  247 loss:  0.10155551135540009\n",
            "epoch:  248 loss:  0.1014220267534256\n",
            "epoch:  249 loss:  0.10128871351480484\n",
            "epoch:  250 loss:  0.10116136074066162\n",
            "epoch:  251 loss:  0.10103078931570053\n",
            "epoch:  252 loss:  0.10089832544326782\n",
            "epoch:  253 loss:  0.10076643526554108\n",
            "epoch:  254 loss:  0.10064200311899185\n",
            "epoch:  255 loss:  0.10050968080759048\n",
            "epoch:  256 loss:  0.10037757456302643\n",
            "epoch:  257 loss:  0.10024809837341309\n",
            "epoch:  258 loss:  0.10012193769216537\n",
            "epoch:  259 loss:  0.09999030828475952\n",
            "epoch:  260 loss:  0.09985890239477158\n",
            "epoch:  261 loss:  0.09973286092281342\n",
            "epoch:  262 loss:  0.0996054857969284\n",
            "epoch:  263 loss:  0.09947453439235687\n",
            "epoch:  264 loss:  0.09934376180171967\n",
            "epoch:  265 loss:  0.09921953082084656\n",
            "epoch:  266 loss:  0.09909075498580933\n",
            "epoch:  267 loss:  0.09896048158407211\n",
            "epoch:  268 loss:  0.09883047640323639\n",
            "epoch:  269 loss:  0.09870850294828415\n",
            "epoch:  270 loss:  0.09857945889234543\n",
            "epoch:  271 loss:  0.09844999015331268\n",
            "epoch:  272 loss:  0.09832189977169037\n",
            "epoch:  273 loss:  0.09819921851158142\n",
            "epoch:  274 loss:  0.09807013720273972\n",
            "epoch:  275 loss:  0.09794126451015472\n",
            "epoch:  276 loss:  0.09781501442193985\n",
            "epoch:  277 loss:  0.09769182652235031\n",
            "epoch:  278 loss:  0.09756340086460114\n",
            "epoch:  279 loss:  0.09743575006723404\n",
            "epoch:  280 loss:  0.09731119126081467\n",
            "epoch:  281 loss:  0.09718775004148483\n",
            "epoch:  282 loss:  0.09706003963947296\n",
            "epoch:  283 loss:  0.09693249315023422\n",
            "epoch:  284 loss:  0.09680904448032379\n",
            "epoch:  285 loss:  0.09668560326099396\n",
            "epoch:  286 loss:  0.09655849635601044\n",
            "epoch:  287 loss:  0.09643158316612244\n",
            "epoch:  288 loss:  0.09630955755710602\n",
            "epoch:  289 loss:  0.09618675708770752\n",
            "epoch:  290 loss:  0.0960603579878807\n",
            "epoch:  291 loss:  0.09593408554792404\n",
            "epoch:  292 loss:  0.09581252932548523\n",
            "epoch:  293 loss:  0.09568975120782852\n",
            "epoch:  294 loss:  0.09556397795677185\n",
            "epoch:  295 loss:  0.09543837606906891\n",
            "epoch:  296 loss:  0.09531737864017487\n",
            "epoch:  297 loss:  0.09519539028406143\n",
            "epoch:  298 loss:  0.09507094323635101\n",
            "epoch:  299 loss:  0.09494604915380478\n",
            "epoch:  300 loss:  0.094825379550457\n",
            "epoch:  301 loss:  0.09470422565937042\n",
            "epoch:  302 loss:  0.09457974880933762\n",
            "epoch:  303 loss:  0.09445545077323914\n",
            "epoch:  304 loss:  0.09433495998382568\n",
            "epoch:  305 loss:  0.09421499073505402\n",
            "epoch:  306 loss:  0.09409116208553314\n",
            "epoch:  307 loss:  0.09396760910749435\n",
            "epoch:  308 loss:  0.09384752064943314\n",
            "epoch:  309 loss:  0.0937289148569107\n",
            "epoch:  310 loss:  0.09360574185848236\n",
            "epoch:  311 loss:  0.09348277747631073\n",
            "epoch:  312 loss:  0.0933617502450943\n",
            "epoch:  313 loss:  0.0932447537779808\n",
            "epoch:  314 loss:  0.09312222898006439\n",
            "epoch:  315 loss:  0.09299986809492111\n",
            "epoch:  316 loss:  0.09287819266319275\n",
            "epoch:  317 loss:  0.09276314079761505\n",
            "epoch:  318 loss:  0.09264186769723892\n",
            "epoch:  319 loss:  0.09252021461725235\n",
            "epoch:  320 loss:  0.09239865839481354\n",
            "epoch:  321 loss:  0.09228366613388062\n",
            "epoch:  322 loss:  0.09216338396072388\n",
            "epoch:  323 loss:  0.09204232692718506\n",
            "epoch:  324 loss:  0.09192325919866562\n",
            "epoch:  325 loss:  0.0918114185333252\n",
            "epoch:  326 loss:  0.09169894456863403\n",
            "epoch:  327 loss:  0.09158319234848022\n",
            "epoch:  328 loss:  0.09146713465452194\n",
            "epoch:  329 loss:  0.0913538932800293\n",
            "epoch:  330 loss:  0.09124350547790527\n",
            "epoch:  331 loss:  0.09112867712974548\n",
            "epoch:  332 loss:  0.09101318567991257\n",
            "epoch:  333 loss:  0.09089836478233337\n",
            "epoch:  334 loss:  0.09079034626483917\n",
            "epoch:  335 loss:  0.09067641198635101\n",
            "epoch:  336 loss:  0.09056150913238525\n",
            "epoch:  337 loss:  0.09044673293828964\n",
            "epoch:  338 loss:  0.09033732116222382\n",
            "epoch:  339 loss:  0.09022633731365204\n",
            "epoch:  340 loss:  0.09011201560497284\n",
            "epoch:  341 loss:  0.08999785780906677\n",
            "epoch:  342 loss:  0.08988626301288605\n",
            "epoch:  343 loss:  0.0897781252861023\n",
            "epoch:  344 loss:  0.08966473489999771\n",
            "epoch:  345 loss:  0.08955111354589462\n",
            "epoch:  346 loss:  0.08943761885166168\n",
            "epoch:  347 loss:  0.08933154493570328\n",
            "epoch:  348 loss:  0.08921970427036285\n",
            "epoch:  349 loss:  0.08910662680864334\n",
            "epoch:  350 loss:  0.08899369835853577\n",
            "epoch:  351 loss:  0.08888445794582367\n",
            "epoch:  352 loss:  0.08877675235271454\n",
            "epoch:  353 loss:  0.08866424113512039\n",
            "epoch:  354 loss:  0.08855187892913818\n",
            "epoch:  355 loss:  0.08843972533941269\n",
            "epoch:  356 loss:  0.08833564817905426\n",
            "epoch:  357 loss:  0.08822412043809891\n",
            "epoch:  358 loss:  0.08811229467391968\n",
            "epoch:  359 loss:  0.08800063282251358\n",
            "epoch:  360 loss:  0.08789287507534027\n",
            "epoch:  361 loss:  0.08778605610132217\n",
            "epoch:  362 loss:  0.08767484128475189\n",
            "epoch:  363 loss:  0.08756376802921295\n",
            "epoch:  364 loss:  0.08745278418064117\n",
            "epoch:  365 loss:  0.08734951168298721\n",
            "epoch:  366 loss:  0.08723951876163483\n",
            "epoch:  367 loss:  0.08712895214557648\n",
            "epoch:  368 loss:  0.08701853454113007\n",
            "epoch:  369 loss:  0.08691106736660004\n",
            "epoch:  370 loss:  0.08680637180805206\n",
            "epoch:  371 loss:  0.08669634908437729\n",
            "epoch:  372 loss:  0.08658647537231445\n",
            "epoch:  373 loss:  0.0864768847823143\n",
            "epoch:  374 loss:  0.08637292683124542\n",
            "epoch:  375 loss:  0.08626579493284225\n",
            "epoch:  376 loss:  0.086156465113163\n",
            "epoch:  377 loss:  0.08604733645915985\n",
            "epoch:  378 loss:  0.08593917638063431\n",
            "epoch:  379 loss:  0.08583720028400421\n",
            "epoch:  380 loss:  0.08572865277528763\n",
            "epoch:  381 loss:  0.0856199786067009\n",
            "epoch:  382 loss:  0.08551166206598282\n",
            "epoch:  383 loss:  0.08540606498718262\n",
            "epoch:  384 loss:  0.08530287444591522\n",
            "epoch:  385 loss:  0.08519478142261505\n",
            "epoch:  386 loss:  0.08508682250976562\n",
            "epoch:  387 loss:  0.08497978746891022\n",
            "epoch:  388 loss:  0.08487575501203537\n",
            "epoch:  389 loss:  0.08477161079645157\n",
            "epoch:  390 loss:  0.08466418087482452\n",
            "epoch:  391 loss:  0.0845569521188736\n",
            "epoch:  392 loss:  0.0844511017203331\n",
            "epoch:  393 loss:  0.08434843271970749\n",
            "epoch:  394 loss:  0.08424367010593414\n",
            "epoch:  395 loss:  0.08413691073656082\n",
            "epoch:  396 loss:  0.08403091132640839\n",
            "epoch:  397 loss:  0.08392508327960968\n",
            "epoch:  398 loss:  0.083824023604393\n",
            "epoch:  399 loss:  0.08371898531913757\n",
            "epoch:  400 loss:  0.08361288160085678\n",
            "epoch:  401 loss:  0.08350806683301926\n",
            "epoch:  402 loss:  0.08340238779783249\n",
            "epoch:  403 loss:  0.08330263942480087\n",
            "epoch:  404 loss:  0.08319751173257828\n",
            "epoch:  405 loss:  0.08309239149093628\n",
            "epoch:  406 loss:  0.08298803120851517\n",
            "epoch:  407 loss:  0.08288292586803436\n",
            "epoch:  408 loss:  0.08278421312570572\n",
            "epoch:  409 loss:  0.08267929404973984\n",
            "epoch:  410 loss:  0.08257528394460678\n",
            "epoch:  411 loss:  0.08247111737728119\n",
            "epoch:  412 loss:  0.08236672729253769\n",
            "epoch:  413 loss:  0.08226847648620605\n",
            "epoch:  414 loss:  0.08216425776481628\n",
            "epoch:  415 loss:  0.08206130564212799\n",
            "epoch:  416 loss:  0.0819573923945427\n",
            "epoch:  417 loss:  0.08185356110334396\n",
            "epoch:  418 loss:  0.08175599575042725\n",
            "epoch:  419 loss:  0.08165266364812851\n",
            "epoch:  420 loss:  0.08155010640621185\n",
            "epoch:  421 loss:  0.08144679665565491\n",
            "epoch:  422 loss:  0.08134360611438751\n",
            "epoch:  423 loss:  0.08124634623527527\n",
            "epoch:  424 loss:  0.08114434778690338\n",
            "epoch:  425 loss:  0.0810420885682106\n",
            "epoch:  426 loss:  0.080939382314682\n",
            "epoch:  427 loss:  0.08083681762218475\n",
            "epoch:  428 loss:  0.08073947578668594\n",
            "epoch:  429 loss:  0.08063910901546478\n",
            "epoch:  430 loss:  0.08053720742464066\n",
            "epoch:  431 loss:  0.08043514937162399\n",
            "epoch:  432 loss:  0.08033321052789688\n",
            "epoch:  433 loss:  0.0802355408668518\n",
            "epoch:  434 loss:  0.08013692498207092\n",
            "epoch:  435 loss:  0.08003537356853485\n",
            "epoch:  436 loss:  0.07993397116661072\n",
            "epoch:  437 loss:  0.07983271032571793\n",
            "epoch:  438 loss:  0.0797346979379654\n",
            "epoch:  439 loss:  0.07963765412569046\n",
            "epoch:  440 loss:  0.07953671365976334\n",
            "epoch:  441 loss:  0.07943591475486755\n",
            "epoch:  442 loss:  0.07933525741100311\n",
            "epoch:  443 loss:  0.07923658937215805\n",
            "epoch:  444 loss:  0.07914142310619354\n",
            "epoch:  445 loss:  0.07904113829135895\n",
            "epoch:  446 loss:  0.0789409726858139\n",
            "epoch:  447 loss:  0.0788409411907196\n",
            "epoch:  448 loss:  0.07874162495136261\n",
            "epoch:  449 loss:  0.07864798605442047\n",
            "epoch:  450 loss:  0.07854859530925751\n",
            "epoch:  451 loss:  0.0784490555524826\n",
            "epoch:  452 loss:  0.07834967970848083\n",
            "epoch:  453 loss:  0.07825113832950592\n",
            "epoch:  454 loss:  0.07815596461296082\n",
            "epoch:  455 loss:  0.0780591294169426\n",
            "epoch:  456 loss:  0.07796020805835724\n",
            "epoch:  457 loss:  0.07786141335964203\n",
            "epoch:  458 loss:  0.07776360213756561\n",
            "epoch:  459 loss:  0.07766672968864441\n",
            "epoch:  460 loss:  0.07757271826267242\n",
            "epoch:  461 loss:  0.07747440785169601\n",
            "epoch:  462 loss:  0.07737623900175095\n",
            "epoch:  463 loss:  0.07727911323308945\n",
            "epoch:  464 loss:  0.07718134671449661\n",
            "epoch:  465 loss:  0.07708816975355148\n",
            "epoch:  466 loss:  0.07699157297611237\n",
            "epoch:  467 loss:  0.07689406722784042\n",
            "epoch:  468 loss:  0.07679760456085205\n",
            "epoch:  469 loss:  0.07670038193464279\n",
            "epoch:  470 loss:  0.07660508900880814\n",
            "epoch:  471 loss:  0.07651177793741226\n",
            "epoch:  472 loss:  0.07641483098268509\n",
            "epoch:  473 loss:  0.07631900161504745\n",
            "epoch:  474 loss:  0.07622238993644714\n",
            "epoch:  475 loss:  0.0761258453130722\n",
            "epoch:  476 loss:  0.07603392750024796\n",
            "epoch:  477 loss:  0.0759386345744133\n",
            "epoch:  478 loss:  0.07584340870380402\n",
            "epoch:  479 loss:  0.07574736326932907\n",
            "epoch:  480 loss:  0.0756513923406601\n",
            "epoch:  481 loss:  0.07555682957172394\n",
            "epoch:  482 loss:  0.07546539604663849\n",
            "epoch:  483 loss:  0.07537074387073517\n",
            "epoch:  484 loss:  0.07527531683444977\n",
            "epoch:  485 loss:  0.07517994940280914\n",
            "epoch:  486 loss:  0.07508470118045807\n",
            "epoch:  487 loss:  0.07499291002750397\n",
            "epoch:  488 loss:  0.07490095496177673\n",
            "epoch:  489 loss:  0.07480617612600327\n",
            "epoch:  490 loss:  0.07471144199371338\n",
            "epoch:  491 loss:  0.07461677491664886\n",
            "epoch:  492 loss:  0.07452225685119629\n",
            "epoch:  493 loss:  0.07443376630544662\n",
            "epoch:  494 loss:  0.07433995604515076\n",
            "epoch:  495 loss:  0.07424578070640564\n",
            "epoch:  496 loss:  0.07415173947811127\n",
            "epoch:  497 loss:  0.07405783236026764\n",
            "epoch:  498 loss:  0.07396593689918518\n",
            "epoch:  499 loss:  0.07387667894363403\n",
            "epoch:  500 loss:  0.07378308475017548\n"
          ]
        }
      ]
    }
  ]
}
